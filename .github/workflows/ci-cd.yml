name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      immediate_cleanup:
        description: "Trigger cleanup immediately"
        required: false
        default: "false"

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY_BACKEND: digital-store/backend
  ECR_REPOSITORY_FRONTEND: digital-store/frontend
  IMAGE_TAG: ${{ github.sha }}
  EKS_CLUSTER_NAME: digital-store-cluster

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Validate Workflow Files
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          for file in .github/workflows/*.yml; do
            gh workflow view "$file" >/dev/null
          done

  app-test:
    needs: validate
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '17'
      - name: Build and Test
        run: |
          cd application
          mvn test
      - name: Verify Test Coverage
        run: |
          cd application
          mvn verify

  app-security:
    needs: validate
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: OWASP Dependency Check Backend
        uses: dependency-check/Dependency-Check_Action@main
        env:
          JAVA_HOME: /opt/jdk
        with:
          project: "Application Backend"
          path: "application"
          format: "HTML"
          args: "--failOnCVSS 7"
      - name: OWASP Dependency Check Frontend
        uses: dependency-check/Dependency-Check_Action@main
        env:
          JAVA_HOME: /opt/jdk
        with:
          project: "Application Frontend"
          path: "application/frontend"
          format: "HTML"
          args: "--failOnCVSS 7"
      - name: Build and Test Docker Images
        continue-on-error: true
        run: |
          cd application
          docker build -t backend:latest .
          docker run --rm backend:latest mvn test
          cd frontend
          docker build -t frontend:latest .
          docker run --rm frontend:latest npm test

  security-checks:
    needs: [app-test, app-security]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run TFSec on Terraform
        uses: aquasecurity/tfsec-action@v1.0.0
        with:
          working_directory: terraform
        continue-on-error: true
      - name: Run Semgrep
        uses: returntocorp/semgrep-action@v1
        with:
          config: "auto"
        continue-on-error: true
      - name: Run Gitleaks
        continue-on-error: true
        uses: gitleaks/gitleaks-action@v2
        
      - name: Install Grype
        run: |
          curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin
      - name: Scan backend image with Grype
        continue-on-error: true
        run: |
          docker build -t backend-temp:latest ./application
          grype backend-temp:latest --fail-on high

  terraform-deploy:
    needs: security-checks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: hashicorp/setup-terraform@v1
        with:
          terraform_version: 1.5.0
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Install Helm for Terraform provider
      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod +x get_helm.sh
          ./get_helm.sh
          helm version
          mkdir -p ~/.helm/repository

      # Étape 1: Déployer l'infrastructure avec Terraform
      - name: Get db_password from AWS SSM
        id: get-db-password
        run: |
          DB_PASSWORD=$(aws ssm get-parameter \
            --name "/terraform/db_password" \
            --with-decryption \
            --query "Parameter.Value" \
            --output text)
          echo "TF_VAR_db_password=$DB_PASSWORD" >> $GITHUB_ENV

      - name: Terraform Init & Apply
        run: |
          cd terraform
          terraform init
          terraform plan -out=tfplan
          terraform apply tfplan

      # Étape 2: Login à ECR et push des images
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Get ECR Registry
        run: |
          ECR_REGISTRY=$(aws ecr get-authorization-token --output text --query 'authorizationData[].proxyEndpoint' | sed 's|https://||')
          echo "ECR_REGISTRY=$ECR_REGISTRY" >> $GITHUB_ENV

      - name: Build and push backend image
        env:
          ECR_REGISTRY: ${{ env.ECR_REGISTRY }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY_BACKEND:$IMAGE_TAG -t $ECR_REGISTRY/$ECR_REPOSITORY_BACKEND:latest ./application
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_BACKEND:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_BACKEND:latest

      - name: Build and push frontend image
        env:
          ECR_REGISTRY: ${{ env.ECR_REGISTRY }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY_FRONTEND:$IMAGE_TAG -t $ECR_REGISTRY/$ECR_REPOSITORY_FRONTEND:latest ./application/frontend
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_FRONTEND:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY_FRONTEND:latest

      - name: Verify ECR Images
        run: |
          aws ecr describe-images --repository-name $ECR_REPOSITORY_BACKEND --query 'imageDetails[*].imageTags[?contains(@, `$IMAGE_TAG`)]'
          aws ecr describe-images --repository-name $ECR_REPOSITORY_FRONTEND --query 'imageDetails[*].imageTags[?contains(@, `$IMAGE_TAG`)]'

  eks-deploy:
    needs: terraform-deploy
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Install Helm
      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod +x get_helm.sh
          ./get_helm.sh
          helm version

      - name: Check EKS Cluster Status
        run: |
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.status' --output text)
          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "EKS Cluster is not active. Current status: $CLUSTER_STATUS"
            exit 1
          fi
          echo "EKS Cluster is ACTIVE and ready"

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Get ECR Registry
        run: |
          ECR_REGISTRY=$(aws ecr get-authorization-token --output text --query 'authorizationData[].proxyEndpoint' | sed 's|https://||')
          echo "ECR_REGISTRY=$ECR_REGISTRY" >> $GITHUB_ENV

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Verify EKS Cluster Access
        run: |
          kubectl get nodes
          kubectl get namespaces

      # Wait for the cluster to be fully ready
      - name: Wait for EKS cluster to be fully ready
        run: |
          echo "Waiting for all nodes to be ready..."
          until kubectl get nodes | grep -v "NotReady" | grep -v NAME; do
            echo "Some nodes are not ready yet. Waiting..."
            sleep 10
          done
          echo "All nodes are now ready"
          
          echo "Waiting for core components..."
          kubectl wait --for=condition=available --timeout=300s deployment -l k8s-app=kube-dns -n kube-system
          kubectl wait --for=condition=available --timeout=300s deployment -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system || true
          echo "Core components check completed"

      - name: Deploy to EKS
        env:
          IMAGE_TAG: ${{ github.sha }}
          ECR_REGISTRY: ${{ env.ECR_REGISTRY }}
        run: |
          sed -i "s|image:.*digital-store/backend:.*|image: $ECR_REGISTRY/digital-store/backend:$IMAGE_TAG|g" k8s/backend/deployment.yaml
          sed -i "s|image:.*digital-store/frontend:.*|image: $ECR_REGISTRY/digital-store/frontend:$IMAGE_TAG|g" k8s/frontend/deployment.yaml
          kubectl apply -f k8s

      # Check Helm release status
      - name: Check Helm release status
        run: |
          if kubectl get namespace default; then
            if helm list -n default | grep digital-store; then
              echo "Checking Helm release status..."
              helm status digital-store -n default
            else
              echo "Helm release not found, it may have failed"
            fi
          fi

      - name: Wait for Deployments
        run: |
          kubectl wait --for=condition=available deployment/backend --timeout=300s
          kubectl wait --for=condition=available deployment/frontend --timeout=300s

  validate-infra:
    needs: eks-deploy
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check EKS Cluster Status
        run: |
          CLUSTER_STATUS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.status' --output text)
          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "EKS Cluster is not active. Current status: $CLUSTER_STATUS"
            exit 1
          fi
          echo "EKS Cluster is ACTIVE and ready"

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      # 1. Vérifier que les pods sont prêts
      - name: Verify Pods Status
        run: |
          echo "Waiting for pods to be ready..."
          kubectl wait --for=condition=ready pod -l app=backend --timeout=300s
          kubectl wait --for=condition=ready pod -l app=frontend --timeout=300s
          echo "All pods are ready!"

      # 2. Vérifier les logs des pods
      - name: Verify Application Logs
        run: |
          echo "Checking backend logs..."
          kubectl logs -l app=backend --tail=100
          echo "Checking frontend logs..."
          kubectl logs -l app=frontend --tail=100

      # 3. Vérifier l'Ingress ALB
      - name: Wait for Ingress ALB
        run: |
          echo "Waiting for Ingress ALB to be ready..."
          kubectl wait --for=condition=ready ingress/digital-store-alb -n default --timeout=300s
          echo "Ingress ALB is ready!"

      # 4. Vérifier la santé de l'application via ALB
      - name: Verify application health via ALB
        run: |
          echo "Getting ALB hostname..."
          ALB_HOST=$(kubectl get ingress digital-store-alb -n default -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          if [ -z "$ALB_HOST" ]; then
            echo "Error: ALB hostname not found"
            exit 1
          fi
          echo "ALB URL: http://$ALB_HOST"
          echo "Waiting for ALB to be fully provisioned..."
          sleep 120
          echo "Testing application health..."
          curl --fail http://$ALB_HOST/api/health
          echo "Application health check passed!"


